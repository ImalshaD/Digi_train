{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj-2CkmecHr-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "hDfPabo-faF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roUSc_I7sbt6",
        "outputId": "d1923674-10f8-44ac-b9eb-337de230abb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_autoencoder(input_shape=(1024, 768, 1)):\n",
        "    # Encoder\n",
        "    input_img = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='linear', padding='same')(x)\n",
        "\n",
        "    # Create the autoencoder model\n",
        "    autoencoder = tf.keras.models.Model(input_img, decoded)\n",
        "\n",
        "    # Compile the model\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')  # Use 'mse' for grayscale images\n",
        "\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "5zHqSv7Gca1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(path):\n",
        "    # Extract the file name from the path\n",
        "    filename = os.path.split(path)[1]\n",
        "\n",
        "    # Extract the label from the filename\n",
        "    label = filename.split(\"_\")[:-1]\n",
        "\n",
        "    # Create the target path\n",
        "    target_path = os.path.join(\"/content/gdrive/MyDrive/AutoEncoder/clean\", \"_\".join(label) + \".png\")\n",
        "\n",
        "    # Load the images using PIL\n",
        "    ximg = Image.open(path)\n",
        "    yimg = Image.open(target_path)\n",
        "\n",
        "    # Convert images to arrays\n",
        "    ximg = np.array(ximg)\n",
        "    yimg = np.array(yimg)\n",
        "\n",
        "    # Resize images\n",
        "    ximg = np.array(Image.fromarray(ximg).resize((768, 1024)))\n",
        "    yimg = np.array(Image.fromarray(yimg).resize((768, 1024)))\n",
        "\n",
        "    # Convert images to grayscale\n",
        "    xgrayscale_image = np.array(Image.fromarray(ximg).convert('L'))\n",
        "    ygrayscale_image = np.array(Image.fromarray(yimg).convert('L'))\n",
        "\n",
        "    return xgrayscale_image, ygrayscale_image"
      ],
      "metadata": {
        "id": "5zEJHbC-ck5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = '/content/gdrive/MyDrive/AutoEncoder/shabby'\n",
        "\n",
        "# Get a list of all image file paths\n",
        "image_paths = [os.path.join(data_folder, filename) for filename in os.listdir(data_folder) if filename.endswith('.png')]\n",
        "\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_paths, val_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
        "train_x,train_y,val_x,val_y=[],[],[],[]\n",
        "count=0\n",
        "train_len=len(train_paths)//4\n",
        "for i in range(train_len):\n",
        "    path=train_paths[i]\n",
        "    x,y=load_images(path)\n",
        "    train_x.append(x)\n",
        "    train_y.append(y)\n",
        "    count+=1\n",
        "    print('train:',count/len(train_paths),end=\"\\r\")\n",
        "count=0\n",
        "for path in val_paths:\n",
        "    x,y=load_images(path)\n",
        "    val_x.append(x)\n",
        "    val_y.append(y)\n",
        "    count+=1\n",
        "    print(\"val:\",count/len(val_paths),end=\"\\r\")\n",
        "train_x=np.array(train_x)\n",
        "train_y=np.array(train_y)\n",
        "val_x=np.array(val_x)\n",
        "val_y=np.array(val_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw2eZE2ico2q",
        "outputId": "bf1ce36f-86d8-4da1-a32e-d8d620b41119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_2=[]\n",
        "train_y_2=[]\n",
        "for i in range(train_len,2*train_len):\n",
        "    path=train_paths[i]\n",
        "    x,y=load_images(path)\n",
        "    train_x_2.append(x)\n",
        "    train_y_2.append(y)\n",
        "    count+=1\n",
        "    print('train:',count/len(train_paths),end=\"\\r\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G_G1HRjtPnn",
        "outputId": "1f133fe0-b4ab-4d46-8d0a-e4c4a2904192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_x_2.pkl', 'wb') as file:\n",
        "    pickle.dump(train_x_2, file)\n",
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_y_2.pkl', 'wb') as file:\n",
        "    pickle.dump(train_y_2, file)"
      ],
      "metadata": {
        "id": "0EdPVw0FuAQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_3=[]\n",
        "train_y_3=[]\n",
        "for i in range(train_len*2,3*train_len):\n",
        "    path=train_paths[i]\n",
        "    x,y=load_images(path)\n",
        "    train_x_3.append(x)\n",
        "    train_y_3.append(y)\n",
        "    count+=1\n",
        "    print('train:',count/len(train_paths),end=\"\\r\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cH82s2QuNJ8",
        "outputId": "5eb4e005-302c-4ff7-89a4-bee28ac72b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_x_3.pkl', 'wb') as file:\n",
        "    pickle.dump(train_x_3, file)\n",
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_y_3.pkl', 'wb') as file:\n",
        "    pickle.dump(train_y_3, file)"
      ],
      "metadata": {
        "id": "6gf5RjLSutLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_4=[]\n",
        "train_y_4=[]\n",
        "for i in range(train_len*3,len(train_paths)):\n",
        "    path=train_paths[i]\n",
        "    x,y=load_images(path)\n",
        "    train_x_4.append(x)\n",
        "    train_y_4.append(y)\n",
        "    count+=1\n",
        "    print('train:',count/len(train_paths),end=\"\\r\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCC9XKPcu5P9",
        "outputId": "845cc740-0778-4245-f3e4-2eaa07236516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_x_4.pkl', 'wb') as file:\n",
        "    pickle.dump(train_x_4, file)\n",
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_y_4.pkl', 'wb') as file:\n",
        "    pickle.dump(train_y_4, file)"
      ],
      "metadata": {
        "id": "lFZnhEaovgDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_x_1.pkl', 'wb') as file:\n",
        "    pickle.dump(train_x, file)"
      ],
      "metadata": {
        "id": "DvwtC5_efRhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_y_1.pkl', 'wb') as file:\n",
        "    pickle.dump(train_y, file)"
      ],
      "metadata": {
        "id": "ueQANjgSfjw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_val_x_1.pkl', 'wb') as file:\n",
        "    pickle.dump(val_x, file)"
      ],
      "metadata": {
        "id": "p_QG9W8SfpHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_val_y_1.pkl', 'wb') as file:\n",
        "    pickle.dump(val_y, file)"
      ],
      "metadata": {
        "id": "3WmkXgIQftz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_x_1.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    train_x = pickle.load(file)"
      ],
      "metadata": {
        "id": "kVFh8ahyhOTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_y_1.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    train_y = pickle.load(file)"
      ],
      "metadata": {
        "id": "h1gvqmuchbD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_val_x_1.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    val_x = pickle.load(file)"
      ],
      "metadata": {
        "id": "oaydsXoQhfz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_val_y_1.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    val_y = pickle.load(file)"
      ],
      "metadata": {
        "id": "lLo5Ocv-hlKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder=create_autoencoder()\n",
        "autoencoder.fit(train_x,train_y,epochs=5,batch_size=4,validation_data=(val_x,val_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1s_L9Xws0_9",
        "outputId": "bce99787-a565-440d-dea4-995ae1dc8a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "181/181 [==============================] - 150s 737ms/step - loss: 2714.8506 - val_loss: 3535.3855\n",
            "Epoch 2/5\n",
            "181/181 [==============================] - 130s 721ms/step - loss: 2397.0598 - val_loss: 2449.3860\n",
            "Epoch 3/5\n",
            "181/181 [==============================] - 130s 720ms/step - loss: 2304.8962 - val_loss: 2410.7896\n",
            "Epoch 4/5\n",
            "181/181 [==============================] - 130s 721ms/step - loss: 2260.9800 - val_loss: 2340.8103\n",
            "Epoch 5/5\n",
            "181/181 [==============================] - 130s 720ms/step - loss: 2214.9399 - val_loss: 2274.0979\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ae107f21e70>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.save('/content/gdrive/MyDrive/AutoEncoder/auenc.keras')"
      ],
      "metadata": {
        "id": "0gpExW1w73xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'path/to/your/model.h5' with the actual path to your saved model\n",
        "model_path = '/content/gdrive/MyDrive/AutoEncoder/auenc.keras'\n",
        "autoencoder = keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "qwDNEn98fCuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_x_2.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    train_x_2 = pickle.load(file)\n",
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_train_y_2.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    train_y_2 = pickle.load(file)\n",
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_val_x_1.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    val_x = pickle.load(file)\n",
        "with open('/content/gdrive/MyDrive/AutoEncoder/img_array_val_y_1.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    val_y = pickle.load(file)"
      ],
      "metadata": {
        "id": "NH2cCwL8fPou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_2=np.array(train_x_2)\n",
        "train_y_2=np.array(train_y_2)"
      ],
      "metadata": {
        "id": "gJHHNbCCiDdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(train_x_2,train_y_2,epochs=5,batch_size=4,validation_data=(val_x,val_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zGcpZNifKLB",
        "outputId": "541acc86-4786-4ba3-9114-98fabbdc8a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "181/181 [==============================] - 161s 754ms/step - loss: 2186.2275 - val_loss: 2239.4971\n",
            "Epoch 2/5\n",
            "181/181 [==============================] - 116s 640ms/step - loss: 2143.5889 - val_loss: 2225.2087\n",
            "Epoch 3/5\n",
            "181/181 [==============================] - 129s 713ms/step - loss: 2111.3740 - val_loss: 2229.7981\n",
            "Epoch 4/5\n",
            "181/181 [==============================] - 116s 641ms/step - loss: 2076.2097 - val_loss: 2217.5317\n",
            "Epoch 5/5\n",
            "181/181 [==============================] - 129s 713ms/step - loss: 2044.6608 - val_loss: 2112.7424\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ff1abf51030>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjz4NcRNXOb-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}